# [Project] Autonomous Vehicle #1: Obstacle avoidance

This is part of an autonomous vehicle project that I'm participating in at school. This blog mainly aims to document the code for myself and those who join the project after me.

Here, I will talk about the algorithm and environment used to train the obstacle avoidance model for the vehicle. The model, once trained, will be deployed onto the assembled vehicle.

At the time of writing this blog, the input data for the project is the distance from the vehicle to obstacles, obtained from ultrasonic sensors. To start simply, my team is using the Q-Learning algorithm. In a later blog, I might have switched to using Deep Learning.

## Training Environment

### Simulation

When we started writing the algorithm, our assembled vehicle was not running stably. Therefore, I decided to use a simulation environment for initial training, and then map from simulation to reality. The source code for the simulation can be found [here](https://github.com/minhquang053/autonomous_car/blob/main/QLearning/rl_car_env.py)

The simulation is written using Pygame, a Python library used for game development. Some of the ideas were inspired by [this article](https://medium.com/@sdeleers/autonomous-car-with-reinforcement-learning-part-1-obstacle-avoidance-7c73a2567b7b) (you may need to use a VPN to read it)

Regarding the code, I will provide an explanation step by step.

#### Libraries

```Python
import gym
from gym import spaces
import numpy as np
import math
import pygame
import math
import random
```

Two important libraries used are ```Pygame``` and ```gym```. As mentioned above, ```Pygame``` is used to create the virtual environment for the vehicle to run in. Meanwhile, ```gym``` is an open-source Python library for developing reinforcement learning algorithms by providing an API to support communication between the algorithm and the environment. By combining ```pygame``` with the custom functionality of ```gym```, we can create a unified environment for multiple training algorithms.

Other necessary libraries such as ```math```, ```numpy```, and ```random``` are used to perform essential calculations for the simulation (ensuring basic physics, calculating distances, etc.).

#### Components

##### Car

The car object is created with attributes like position, velocity, steering angle, and methods for movement and collision detection.

##### Sensors

Ultrasonic sensors are simulated to measure the distance from the car to obstacles. This information is used as input data for the model.

##### Obstacle

Obstacle objects are added to the environment. These can be static or dynamic and are used to create challenges for the car to navigate around.

##### Physics Calculations

Libraries such as ```math``` and ```numpy``` are used to perform calculations related to the car's movement and collisions, ensuring accuracy and efficiency.

##### Configuration and Parameters

Parameters such as car size, maximum speed, sensor range, and other environmental factors are configured to simulate a realistic scenario.

#### Map

The map can be created either statically or dynamically on demand. The car will roam freely within the map, trying its best to avoid the obstacles scattered around.

![Training map](/images/2024-01-26-autocar_1/fullmap.png)

### Reinforcement Learning

Reinforcement learning is an approach to machine learning derived from behavioral psychology. In this approach, an agent interacts with the environment and makes decisions to achieve certain goals.

At each time step, the agent observes the current state of the environment and takes an action. This action transitions the environment to a new state and provides a reward to the agent. As this sequence of operations repeats, the agent learns and improves its ability to make optimal decisions.

![qlearning](/images/2024-01-26-autocar_1/qlearning.png)

To implement this, we chose the **Q-Learning** algorithm as our starting point. Essentially, **Q-Learning** is a reinforcement learning technique as described above. Specifically, when the environment provides a reward, we use a **Q-table** to store the cumulative rewards of actions in each state. This allows us to compare and select the optimal action for a given state.

Since there are a lot of detailed [online resources](https://www.datacamp.com/tutorial/introduction-q-learning-beginner-tutorial') comprehensively explain the concepts of reinforcement learning, this blog will focus specifically on our adaptations of the algorithm to suit our project.

#### State representation

In our implementation, we quantize the state space using readings from ultrasonic sensors. Quantization involves discretizing continuous sensor readings into a set number of distinct states. This approach simplifies the representation of the environment and enables the use of tabular methods like Q-Learning, where states are discrete and manageable.

![state](/images/2024-01-26-autocar_1/statespace.png)

#### Action space

We defined a discrete action space: ```turning left```, ```going straight```, ```turning right```. This simplified aproach suits the basic navigation needs of our car.

#### Reward function

Reward is given based on the following criteria:

##### Actions

- Going straight: +0.2 reward
- Turn left/right: -0.1 penalty

##### State

- Improvement: +0.2 reward
- Deterioration: -0.1 penalty
  
##### Stability

- Turning left/right right after turning right/left: -0.8 penalty

##### Safety

- Collisions with an obstacle: -100 penalty

#### Training objectives

1. **Preference for going straight and stability**
   - The car should prioritize going straight and maintaining stability in its movements.

2. **Optimal path selection with minimal collision risk**
   - The car should choose paths that are optimal and minimize the likelihood of colliding with obstacles.

3. **Absolute avoidance of collisions**
   - It is imperative that the car never collides with obstacles under any circumstances.

These objectives provide clear guidelines for training the autonomous car to navigate safely and efficiently in its environment.

#### Training result

![result](/images/2024-01-26-autocar_1/evaluate.png)

The training graph reveals significant results, demonstrating a notable increase in average reward towards the end of the training process. This indicates substantial progress and improvement in the algorithm's performance over time.

### Prototype

Our team has developed prototypes to test our algorithm, deploying it on both a land surface vehicle and a water surface vehicle. This initiative showcases our efforts to explore and validate the algorithm's effectiveness across different types of vehicles and environments.

![blueprint](/images/2024-01-26-autocar_1/blueprint.png)
![prototype](/images/2024-01-26-autocar_1/prototype.jpg)

### Experiment

https://github.com/minhquang053/minhquang053.github.io/assets/100106895/34a65fae-2b39-4aac-8e2f-07e936a1abd9


